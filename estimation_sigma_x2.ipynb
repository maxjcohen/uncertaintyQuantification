{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automotive-brush",
   "metadata": {},
   "source": [
    "# Estimation of $\\Sigma_x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparative-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ucn.models import SMCN\n",
    "from src.trainer import SMCNTrainer\n",
    "from src.dataset import ChunkDataset\n",
    "from src.utils import plot_range, compute_cost, freq_filter, uncertainty_estimation\n",
    "\n",
    "torch.manual_seed(28)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 25,\n",
    "    'figure.figsize': (25, 5)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-people",
   "metadata": {},
   "source": [
    "## Generate noisy dataset from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-mechanism",
   "metadata": {},
   "source": [
    "### Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "honest-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 3\n",
    "d_emb = 2\n",
    "d_out = 1\n",
    "N = 200\n",
    "\n",
    "sigma_y = 0.05\n",
    "sigma_x = 0.1\n",
    "\n",
    "model = SMCN(d_in, d_emb, d_out, n_particles=1)\n",
    "\n",
    "# Sets the range of a few parameters\n",
    "model._f._linear.weight.data = torch.randn(d_out, d_emb) * 0.2 + 0.8\n",
    "model._g.weight_hh.data = torch.randn(d_emb, d_emb) * 0.2\n",
    "model._g.weight_ih.data = torch.randn(d_emb, d_in)*0.2 + 0.8\n",
    "model._sigma_x.data = torch.eye(d_emb)*sigma_x\n",
    "model._sigma_y.data = torch.diag(torch.Tensor([sigma_y]))\n",
    "params_simulation = copy.deepcopy(model.state_dict())\n",
    "\n",
    "T = 50\n",
    "n_samples = 100\n",
    "\n",
    "# Generate random inputs\n",
    "u = torch.randn(T, n_samples, d_in)\n",
    "\n",
    "# Compute outputs with added noise on the hidden state with variance sigma_x\n",
    "with torch.no_grad():\n",
    "    y = model(u, noise=True).view((T, n_samples, d_out))\n",
    "\n",
    "# Add noise on the observations with variance sigma_y\n",
    "y = y + torch.randn(y.shape) * np.sqrt(sigma_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-worth",
   "metadata": {},
   "source": [
    "The dataset is sampled from a model defined as\n",
    "$$\n",
    "\\begin{equation*}\n",
    "        \\left\\{\n",
    "        \\begin{aligned}\n",
    "                x_{t+1} & = \\tanh(W_{xx} x_{t} + W_{xu} u_{t+1} + b_x) + \\eta_{t+1} \\\\\n",
    "                y_{t+1} & = W_y x_{t+1}^L + b_y + \\epsilon_{t+1}                        \\\\\n",
    "        \\end{aligned}\n",
    "        \\right.\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-bullet",
   "metadata": {},
   "source": [
    "### Plot model's prediction on a sample\n",
    "\n",
    "Discrepency between observations and predictions are due to the added observation noise, which the traditionnal model cannot account for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forward-funds",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c51cc804c86d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset = ChunkDataset(y=y.transpose(0, 1).numpy(),\n\u001b[1;32m      5\u001b[0m                       u=u.transpose(0, 1).numpy())\n\u001b[0;32m----> 6\u001b[0;31m dataloader = DataLoader(dataset,\n\u001b[0m\u001b[1;32m      7\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "dataset = ChunkDataset(y=y.transpose(0, 1).numpy(),\n",
    "                      u=u.transpose(0, 1).numpy())\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True\n",
    "                       )\n",
    "\n",
    "# Plot model predictions for a sample\n",
    "for u, y in dataloader:\n",
    "    u = u.transpose(0, 1)\n",
    "    y = y.transpose(0, 1)\n",
    "    break\n",
    "\n",
    "with torch.no_grad():\n",
    "    netout = model(u, noise=False)\n",
    "    \n",
    "netout = netout.numpy().squeeze()\n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "plt.plot(netout[:, 0], label='predictions')\n",
    "plt.plot(y[:, 0, 0], lw=3, label='observations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-glenn",
   "metadata": {},
   "source": [
    "## Estimation by EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SMCN(d_in, d_emb, d_out, n_particles=N)\n",
    "# model.load_state_dict(params_simulation)\n",
    "\n",
    "# Set the number of particules\n",
    "model.N = N\n",
    "\n",
    "# Freeze the model\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train by maximizing log likelihood\n",
    "# This cell will most likely yield a warning from pytorch lightning:\n",
    "#        training_step returned None if it was on purpose, ignore this warning...\n",
    "# This *is* on purpose as we are learning sigma_x by EM, thus not using any gradient\n",
    "train_model = SMCNTrainer(model, lr=1e-2)\n",
    "trainer = pl.Trainer(max_epochs=30, gpus=0)\n",
    "trainer.fit(train_model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.Tensor(train_model._hist['_sigma_x']).squeeze().det().numpy())\n",
    "print(F\"Estimated Sigma_x: {model.sigma_x2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-midnight",
   "metadata": {},
   "source": [
    "### Smoother predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.N = N\n",
    "\n",
    "for u, y in dataloader:\n",
    "    u = u.transpose(0, 1)\n",
    "    y = y.transpose(0, 1)\n",
    "    break\n",
    "\n",
    "with torch.no_grad():\n",
    "    netout = model(u=u,\n",
    "                   y=y,\n",
    "                   noise=True)\n",
    "\n",
    "netout = model.smooth_pms(netout, model.I).numpy()\n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "plot_range(netout[:, 0, :, 0], label='predictions')\n",
    "plt.plot(y[:, 0, 0], lw=3, label='observations')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-nirvana",
   "metadata": {},
   "source": [
    "### Predictions at $t+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filter's prediction for each time step\n",
    "with torch.no_grad():\n",
    "    netout = model(u=u,\n",
    "                   y=y,\n",
    "                   noise=True)\n",
    "\n",
    "netout = netout.squeeze()[:, 0]\n",
    "\n",
    "# netout = netout + torch.randn(netout.shape) * np.sqrt(model.sigma_y.detach())-0.1\n",
    "mean = netout * model.W[:, 0]\n",
    "mean = mean.sum(-1)\n",
    "\n",
    "std = netout.square() * model.W[:, 0]\n",
    "std = std.sum(-1)\n",
    "std = std + model.sigma_y2.detach().numpy() - np.square(mean)\n",
    "std = std.flatten()\n",
    "\n",
    "std = std.numpy()\n",
    "mean = mean.numpy()\n",
    "\n",
    "# plt.figure(figsize=(30, 8))\n",
    "plt.plot(mean, label='predictions')\n",
    "plt.fill_between(np.arange(len(mean)), mean - 3*std, mean + 3*std, alpha=.3)\n",
    "# plt.fill_between(np.arange(len(mean)), mean - 3*std_wrong, mean + 3*std_wrong, alpha=.3)\n",
    "plt.plot(y[:, 0, 0], lw=3, label='observations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "accuracy = (((mean - 3*std) < y[:, 0, 0].numpy()) & (y[:, 0, 0].numpy() < (mean + 3*std))).sum() / mean.shape[0]\n",
    "print(f'Accuracy: {accuracy*100:.0f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-filter",
   "metadata": {},
   "source": [
    "### Plot particules with $\\alpha \\propto \\omega$\n",
    "\n",
    "We plot each trajectory, with a transparancy setting proportionnal to their associated weight. This graph aims at visualizing particle degenerecence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    netout = model(u=u,\n",
    "                   y=y,\n",
    "                   noise=True)\n",
    "\n",
    "netout = model.smooth_pms(netout, model.I)\n",
    "\n",
    "batch_idx = 0\n",
    "\n",
    "netout = netout[:, batch_idx].squeeze().numpy()\n",
    "w = model.w[batch_idx]\n",
    "# w = torch.softmax(w, dim=0)\n",
    "w = w.numpy()\n",
    "\n",
    "# plt.plot(y[:, batch_idx].squeeze().numpy(), '--', lw=3, alpha=0.6, c='green')\n",
    "for trajectory, weight in zip(netout.T, w):\n",
    "    plt.plot(trajectory, alpha=weight, c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-giant",
   "metadata": {},
   "source": [
    "### Compute uncertainty accuracy on the entire dataset\n",
    "\n",
    "Compute prediction at $t+1$, and compute the accuracy score based on wether the observation is inside the predicted interval. Average this score on the entire dataset. We expect a **score close to 0.98%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Uncertainty estimation: {uncertainty_estimation(model, dataloader)*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
